{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b18051-1794-4511-84a7-fc3a614cea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset...\n",
      "Dataset chargé avec succès.\n",
      "Conversion des valeurs cibles en sentiments effectuée.\n",
      "Nettoyage du texte...\n",
      "Nettoyage du texte effectué.\n",
      "Tokenisation des textes...\n",
      "Tokenisation effectuée.\n",
      "Séparation du dataset en Train et Test...\n",
      "Séparation effectuée.\n",
      "Application de CountVectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.789034375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78    159494\n",
      "           1       0.78      0.81      0.79    160506\n",
      "\n",
      "    accuracy                           0.79    320000\n",
      "   macro avg       0.79      0.79      0.79    320000\n",
      "weighted avg       0.79      0.79      0.79    320000\n",
      "\n",
      "Tokenisation pour LSTM...\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 1135s 28ms/step - loss: 0.4303 - accuracy: 0.8016 - val_loss: 0.4081 - val_accuracy: 0.8132\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 2348s 59ms/step - loss: 0.4023 - accuracy: 0.8178 - val_loss: 0.4000 - val_accuracy: 0.8176\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 1012s 25ms/step - loss: 0.3906 - accuracy: 0.8242 - val_loss: 0.3954 - val_accuracy: 0.8199\n",
      "10000/10000 [==============================] - 85s 8ms/step\n",
      "LSTM Accuracy: 0.8199375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82    159494\n",
      "           1       0.82      0.82      0.82    160506\n",
      "\n",
      "    accuracy                           0.82    320000\n",
      "   macro avg       0.82      0.82      0.82    320000\n",
      "weighted avg       0.82      0.82      0.82    320000\n",
      "\n",
      "Chargement du modèle BERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a517317725747cf86af0db11887f74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f74d3549a347aba088d66b9627cbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f17630f43e4d53a1a7a33a33c783c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765b39d80b8f4df7b2d0e4e332ba08be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157a7fc6050740a2a265671b9a04af6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Charger le dataset\n",
    "print(\"Chargement du dataset...\")\n",
    "data = pd.read_csv(\"E:/sentement140/training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', header=None)\n",
    "data.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "print(\"Dataset chargé avec succès.\")\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "data = data[[\"target\", \"text\"]]\n",
    "\n",
    "# Convertir les valeurs cibles en sentiments\n",
    "data['sentiment'] = data['target'].replace({0: \"negative\", 4: \"positive\", 2: \"neutral\"})\n",
    "print(\"Conversion des valeurs cibles en sentiments effectuée.\")\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)     # Supprimer les URLs\n",
    "    text = re.sub(r'@\\w+', '', text)        # Supprimer les mentions\n",
    "    text = re.sub(r'#\\w+', '', text)        # Supprimer les hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)     # Supprimer les caractères spéciaux\n",
    "    text = text.lower()                     # Mettre tout en minuscules\n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "print(\"Nettoyage du texte...\")\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "print(\"Nettoyage du texte effectué.\")\n",
    "\n",
    "# Appliquer la tokenisation sur le texte nettoyé\n",
    "print(\"Tokenisation des textes...\")\n",
    "data['tokens'] = data['cleaned_text'].apply(word_tokenize)\n",
    "print(\"Tokenisation effectuée.\")\n",
    "\n",
    "# Train-Test Split\n",
    "print(\"Séparation du dataset en Train et Test...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['sentiment'].map({'negative': 0, 'positive': 1}), test_size=0.2, random_state=42)\n",
    "print(\"Séparation effectuée.\")\n",
    "\n",
    "# 1. **Model A - Logistic Regression**\n",
    "print(\"Application de CountVectorizer...\")\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_vect, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_vect)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# 2. **Model B - LSTM**\n",
    "print(\"Tokenisation pour LSTM...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=3, batch_size=32)\n",
    "\n",
    "y_pred_lstm = (lstm_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "print(\"LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(classification_report(y_test, y_pred_lstm))\n",
    "\n",
    "# 3. **Model C - BERT**\n",
    "print(\"Chargement du modèle BERT...\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "train_encodings = tokenizer_bert(list(X_train), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "test_encodings = tokenizer_bert(list(X_test), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "bert_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bert_model.fit(train_encodings['input_ids'], y_train, validation_data=(test_encodings['input_ids'], y_test), epochs=2, batch_size=16)\n",
    "\n",
    "y_pred_bert = np.argmax(bert_model.predict(test_encodings['input_ids']).logits, axis=1)\n",
    "print(\"BERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(classification_report(y_test, y_pred_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a46ea-ac67-418a-8ccf-7526d60a9a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
